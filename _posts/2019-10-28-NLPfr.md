---
published: true
title: Traitement Automatique du Langage Naturel en Fran√ßais (TAL)
collection: st
layout: single
author_profile: true
read_time: true
categories: [machinelearning]
excerpt : "Stat4Decision"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
search: false
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

# I. A quoi sert le Traitement Automatique du Langage Naturel (TAL)?

Le traitement du Langage Naturel est un des domaines de recherche les plus actifs en science des donn√©es actuellement. C'est un domaine √† l'intersection du Machine Learning et de la linguistique. 

> Le traitement de langage naturel a pour but d'extraire des informations et une signification d'un contenu textuel.

Le Traitement Automatique du Langage naturel (TAL) ou Natural Language Processing (NLP) en anglais trouve de nombreuses applications dans la vie de tous les jours:
- traduction de texte (DeepL par exemple)
- correcteur orthographique
- r√©sum√© automatique d'un contenu
- synth√®se vocale
- classification de texte
- analyse d'opinion/sentiment
- pr√©diction du prochain mot sur smartphone
- extraction des entit√©s nomm√©es depuis un texte
- ...

La plupart des ressources disponibles actuellement sont en anglais, ce qui implique que la plupart des mod√®les pr√©-entrain√©s sont √©galement sp√©cifiques √† la langue anglaise. Cependant, il existe des librairies et des outils en fran√ßais pour accomplir les t√¢ches mentionn√©es ci-dessus. Nous allons les voir dans cet article.

# II. Les grands principes

Le TAL est g√©n√©ralement compos√© de deux √† trois grandes √©tapes:
- Pr√©-traitement : une √©tape qui cherche √† standardiser du texte afin de rendre son usage plus facile
- Repr√©sentation du texte comme un vecteur : Cette √©tape peut √™tre effectu√©e via des techniques de sac de mots (Bag of Words) ou Term Frequency-Inverse Document Frequency (Tf-IdF). On peut √©galement apprendre des repr√©sentations vectorielles (embedding) par apprentissage profond.
- Classification, trouver la phrase la plus similaire... (optionnel).

Dans cet article, nous allons couvrir les t√¢ches de TAL les plus communes pour lesquelles des outils sp√©cifiques au fran√ßais existent.

Nous utiliserons principalement SpaCy. [SpaCy](https://spacy.io/) est une jeune librairie (2015) qui offre des mod√®les pr√©-entrain√©s pour diverses applications, y compris la reconnaissance d'entit√©s nomm√©es. SpaCy est la principale alternative √† NLTK (Natural Language Tool Kit), la librairie historique pour le TAL avec Python, et propose de nombreuses innovations et options de visualisation qui sont tr√®s int√©ressantes.

Apr√®s avoir install√© la librairie SpaCy (`pip install spacy`), il faut t√©l√©charger les mod√®les fran√ßais.

```bash
python -m spacy download fr_core_news_sm
```

Ce mod√®le est un r√©seau convolutionnel entrain√© sur deux corpus, WikiNER et Sequoia, ce qui repr√©sente de gros volumes de donn√©es en fran√ßais (typiquement plusieurs dizaines de Go).

Dans un notebook Jupyter, on peut alors importer SpaCy et charger le mod√®le fran√ßais.

```python
import spacy
nlp = spacy.load("fr_core_news_sm")
```

On va √©galement cr√©er une phrase d'exemple pour toutes les t√¢ches de TAL que nous allons illustrer.

```python
test = "Bouygues a eu une coupure de r√©seau √† Marseille"
```

# III. TAL en Fran√ßais

## 1. Tokenisation

La tokenisation cherche √† transformer un texte en une s√©rie de tokens individuels. Dans l'id√©e, chaque token repr√©sente un mot, et identifier des mots semble √™tre une t√¢che relativement simple. Mais comment g√©rer en fran√ßais des exemples tels que: "J'ai froid". Il faut que le mod√®le de tokenisation s√©pare le "J'" comme √©tant un premier mot.

SpaCy offre une fonctionnalit√© de tokenisation en utilisant la fonction `nlp`. Cette fonction est le point d'entr√©e vers toutes les fonctionnalit√©s de SpaCy. Il sert √† repr√©senter le texte sous une forme interpr√©table par la librairie.

```python
def return_token(sentence):
    # Tokeniser la phrase
    doc = nlp(sentence)
    # Retourner le texte de chaque token
    return [X.text for X in doc]
```

Ainsi, en appliquant cette tokenisation √† notre phrase, on obtient:

```python
return_token(test)
```

`['Bouygues', 'a', 'eu', 'une', 'coupure', 'de', 'r√©seau', '√†', 'Marseille']`

## 2. Enlever les mots les plus fr√©quents

Certains mots se retrouvent tr√®s fr√©quemment dans la langue fran√ßaise. En anglais, on les appelle les "stop words". Ces mots, bien souvent, n'apportent pas d'information dans les t√¢ches suivantes. Lorsque l'on effectue par exemple une classification par la m√©thode Tf-IdF, on souhaite limiter la quantit√© de mots dans les donn√©es d'entra√Ænement.

Les "stop words" sont √©tablis comme des listes de mots. Ces listes sont g√©n√©ralement disponibles dans une librairie appel√©e NLTK (Natural Language Tool Kit), et dans beaucoup de langues diff√©rentes. On acc√®de aux listes en fran√ßais de cette mani√®re:

```python
from nltk.corpus import stopwords
stopWords = set(stopwords.words('french'))
```

```python
{'ai',
 'aie',
 'aient',
 'aies',
 'ait',
 'as',
 'au',
 'aura',
 'aurai',
 'auraient',
 'aurais',
 ...
```

Pour filtrer le contenu de la phrase, on enl√®ve tous les mots pr√©sents dans cette liste:

```python
clean_words = []
for token in return_token(test):
    if token not in stopWords:
        clean_words.append(token)

clean_words
```

```
['Bouygues', 'a', 'coupure', 'r√©seau', 'Marseille', '.']
```

## 3. Tokenisation par phrases

On peut √©galement appliquer une tokenisation par phrase afin d'identifier les diff√©rentes phrases d'un texte. Cette √©tape peut √† nouveau sembler facile, puisqu'a priori, il suffit de couper chaque phrase lorsqu'un point est rencontr√© (ou un point d'exclamation ou d'interrogation).

Mais que se passerait-t-il dans ce cas-l√†?

`Bouygues a eu une coupure de r√©seau √† Marseille. La panne a affect√© 300.000 utilisateurs.`

Il faut donc une compr√©hension du contexte afin d'effectuer une bonne tokenisation par phrase.

```python
def return_token_sent(sentence):
    # Tokeniser la phrase
    doc = nlp(sentence)
    # Retourner le texte de chaque phrase
    return [X.text for X in doc.sents]
```

On applique alors la tokenisation √† la phrase mentionn√©e pr√©c√©demment.

```python
return_token_sent("Bouygues a eu une coupure de r√©seau √† Marseille. La panne a affect√© 300.000 utilisateurs.")
```

La tokenisation des phrases retourne alors:

```
['Bouygues a eu une coupure de r√©seau √† Marseille.',
 'La panne a affect√© 300.000 utilisateurs.']
```

## 4. Stemming

Le stemming consiste √† r√©duire un mot dans sa forme "racine". Le but du stemming est de regrouper de nombreuses variantes d'un mot comme un seul et m√™me mot. Par exemple, une fois que l'on applique un stemming sur "Chiens" ou "Chien", le mot r√©sultant est le m√™me. Cela permet notamment de r√©duire la taille du vocabulaire dans les approches de type sac de mots ou Tf-IdF. 

Un des stemmers les plus connus est le Snowball Stemmer. Ce stemmer est diponible en fran√ßais.

```python
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer(language='french')

def return_stem(sentence):
    doc = nlp(sentence)
    return [stemmer.stem(X.text) for X in doc]
```

Si on applique ce stemmer √† notre phrase d'exemple:

```python
return_stem(test)
```


```python
['bouygu', 'a', 'eu', 'une', 'coupur', 'de', 'r√©seau', '√†', 'marseil', '.']
```

## 5. Reconnaissance d'entit√©s nomm√©es (NER)

La reconnaissance d'entit√©s nomm√©es cherche √† d√©tecter les entit√©s telles que des personnes, des entreprises ou des lieux dans un texte. Cela s'effectue tr√®s facilement avec SpaCy.


```python
def return_NER(sentence):
    # Tokeniser la phrase
    doc = nlp(sentence)
    # Retourner le texte et le label pour chaque entit√©
    return [(X.text, X.label_) for X in doc.ents]
```

Puis on peut appliquer la fonction √† une phrase d'exemple.

```python
return_NER(test)
```

Les entit√©s identifi√©es sont les suivantes:

```
[('Bouygues', 'ORG'), ('Marseille', 'LOC')]
```

Bouygues est reconnue comme une organisation, et Marseille comme un lieu.

Spacy offre des "Visualizers", des outils graphiques qui permettent d'afficher les r√©sultats de reconnaissances d'entit√©s nomm√©es ou d'√©tiquetage par exemple. 

```python
from spacy import displacy

doc = nlp(test)
displacy.render(doc, style="ent", jupyter=True)
```

![image](https://maelfabien.github.io/assets/images/s4d2_0.png)

On peut √©galement pr√©ciser des options √† DisplaCy pour n'afficher que les organisations, d'une certaine couleur:

```python
doc = nlp(test)
colors = {"ORG": "linear-gradient(90deg, #aa9cfc, #fc9ce7)"}
options = {"ents": ["ORG"], "colors": colors}

displacy.render(doc, style="ent", jupyter=True, options=options)
```

![image](https://maelfabien.github.io/assets/images/s4d2_1.png)

## 6. L‚Äô√©tiquetage morpho-syntaxique

L‚Äô√©tiquetage morpho-syntaxique ou Part-of-Speech (POS) Tagging en anglais essaye d'attribuer une √©tiquette √† chaque mot d'une phrase mentionnant la fonctionnalit√© grammaticale d'un mot (Nom propre, adjectif, d√©terminant...).

```python
def return_POS(sentence):
    # Tokeniser la phrase
    doc = nlp(sentence)
    # Retourner les √©tiquettes de chaque token
    return [(X, X.pos_) for X in doc]
```

Les √©tiquettes identifi√©es sont les suivants:

```python
return_POS(test)
```

```
[(Bouygues, 'ADJ'),
 (a, 'AUX'),
 (eu, 'VERB'),
 (une, 'DET'),
 (coupure, 'NOUN'),
 (de, 'ADP'),
 (r√©seau, 'NOUN'),
 (√†, 'ADP'),
 (Marseille, 'PROPN')]
```

On remarque que Bouygues n'est pas identifi√© comme un nom propre, mais comme un Adjectif. Pour le reste, les tags sont corrects.

Spacy dispose √©galement d'une option de visualisation qui nous permet simplement d'afficher les √©tiquettes identifi√©es ainsi que les d√©pendances entre ces √©tiquettes.

```python
doc = nlp(test)
displacy.serve(doc, style="dep")
```

![image](https://maelfabien.github.io/assets/images/s4d2_2.png)

## 7. Embedding par mot

Avec SpaCy, on peut facilement r√©cup√©rer le vecteur correspondant √† chaque mot une fois pass√© dans le mod√®le pr√©-entrain√© en fran√ßais.

Cela nous sert √† repr√©senter chaque mot comme √©tant un vecteur de taille 96.

```python
import numpy as np

def return_word_embedding(sentence):
    # Tokeniser la phrase
    doc = nlp(sentence)
    # Retourner le vecteur li√© √† chaque token
    return [(X.vector) for X in doc]
```

Ainsi, lorsqu'appliqu√© √† la phrase test, on obtient:

```python
return_word_embedding(test)
```

```
[array([ -1.8685186 ,   1.3645297 ,  -2.3505871 ,  -1.233012  ,
         -3.702136  ,   1.3316352 ,  -1.3532144 ,  -3.879726  ,
         -7.051861  ,  -2.8570302 ,  -2.409908  ,   3.3500502 ,
          3.8512042 ,  -0.5462021 ,  -1.7187259 ,  -5.341373  ,
          ...
```

Cette information nous sert notamment lorsque l'on cherche √† caract√©riser la similarit√© entre deux mots ou deux phrases. 

## 8. Similarit√© entre phrases

Afin de d√©terminer la similarit√© entre deux phrases, nous allons opter pour une m√©thode tr√®s simple :
- d√©terminer l'embedding moyen d'une phrase en moyennant l'embedding de tous les mots de la phrase
- calculer la distance entre deux phrases par simple distance euclidienne

Cela s'effectue tr√®s facilement avec SpaCy !

```python
def return_mean_embedding(sentence):
    # Tokeniser la phrase
    doc = nlp(sentence)
    # Retourner la moyenne des vecteurs pour chaque phrase
    return np.mean([(X.vector) for X in doc], axis=0)
```

On peut alors tester notre fonction avec plusieurs phrases:

```python
test_2 = "Le r√©seau sera bientot r√©tabli √† Marseille"
test_3 = "La panne r√©seau affecte plusieurs utilisateurs de l'op√©rateur"
test_4 = "Il fait 18 degr√©s ici"
```

Ici, on s'attend √† ce que la phrase 2 soit la plus proche de la phrase test, puis la phrase 3 et 4.

Avec Numpy, la distance euclidienne se calcule simplement :

```python
np.linalg.norm(return_tensor(test)-return_tensor(test_2))
np.linalg.norm(return_tensor(test)-return_tensor(test_3))
np.linalg.norm(return_tensor(test)-return_tensor(test_4))
```

On obtient alors les r√©sultats suivants:

```python
16.104986
17.035103
22.039303
```

La phrase 2 est bien identifi√©e comme la plus proche, puis la phrase 3 et 4. On peut √©galement utiliser cette approche pour classifier si une phrase appartient √† une classe ou √† une autre.

## 9. Transformers ü§ó

Si vous avez suivi l'actualit√© du TAL ces derniers mois, vous avez surement entendu parl√© des Transformers, des mod√®les √©tat-de-l'art qui apprennent des repr√©sentations vectorielles √† partir d'un texte d'entr√©e et qui d√©passent les capacit√©s humaines sur certains points. Ces mod√®les peuvent √™tre pr√©-entra√Æn√©s et sont mis √† disposition par Hugging Face, une startup sp√©cialis√©e dans le traitement de langage naturel.

Les mod√®les les plus r√©cents mis √† disposition par Hugging Face sp√©cifiques au fran√ßais sont entra√Æn√©s avec XLM, une architecture qui a depuis √©t√© battue par de nombreux autres mod√®les, dont BERT. 

BERT est √©galement disponible dans une version multi-linguage, entrain√© sur le Wikipedia de plus de 104 langues, sous le nom de : `bert-base-multilingual-cased`.

Les transformers sont particuli√®rement bons pour:
- la g√©n√©ration de texte
- apprendre une repr√©sentation vectorielle
- pr√©dire si une phrase est la suite d'une autre
- les t√¢ches de questions/r√©ponses

Nous allons ici donner un exemple de comment utiliser les transformers pour pr√©dire si une phrase est la suite d'une autre. On trouve des applications de ceci lorsque l'on cherche par exemple √† segmenter des fils de discussions en plusieurs blocs distincts. 

On suppose ici que PyTorch et Transformers sont install√©s (`pip install transformers`). 

```python
import torch
from transformers import *
```

Ensuite, on doit charger un tokenizer et un mod√®le.

```python
# Tokeniser, Model
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertForNextSentencePrediction.from_pretrained('bert-base-multilingual-cased')
model.eval()
```

La ligne `model.eval()` permet de passer le mod√®le en mode √©valuation.

Par la suite, on passe au mod√®le une s√©quence de texte convertie en tokens, et des d√©limitations des phrases:

```python
text = "Comment √ßa va ? Bien merci, un peu stress√© avant l'examen"
# Texte tokenis√©
tokenized_text = tokenizer.tokenize(text)
# Convertir le texte en indexs
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

On transforme le tout en tenseurs interpr√©tables par le mod√®le:

```python
# Transformer en tenseurs
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
```

Finalement, on pr√©dit si la deuxi√®me phrase est la suite de la premi√®re:

```python
predictions = model(tokens_tensor, segments_tensors)
```

Si l'√©l√©ment en position 0 est plus grand que celui en position 1, la phrase est la suite de la premi√®re. Autrement, c'est une nouvelle discussion qui commence.

```python
if np.argmax(predictions) == 0:
    print("Suite")
else:
    print("Pas la suite")
```

# Conclusion

Nous avons couvert dans cet article de nombreuses applications de Traitement Automatique du Langage Naturel en Fran√ßais. Bien que de tr√®s nombreuses ressources existent exclusivement en Anglais, il existe tout de m√™me des outils int√©ressants pour le Fran√ßais, que votre probl√®me de TAL concerne une classification de texte par Tf-IdF, ou une approche par Deep Learning ou Transformers.

Liens utiles:
- [SpaCy](https://spacy.io/)
- [NLTK](https://www.nltk.org/)
- [Hugging Face](https://huggingface.co/)